"""
absl -  for building Python applications.
absl.flags - defines a distributed command line system, replacing systems like getopt(), optparse, and manual argument processing

# tensorflow ì˜ flag =  ê³ ì •ê°’ìœ¼ë¡œ ë˜ì–´ ìˆëŠ” ê¸°ë³¸ì ì¸ ë°ì´í„°ë¥¼ í¸ë¦¬í•˜ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
int, float, boolean, string ì˜ ê°’ì„ ì €ì¥í•˜ê³ , ê°€ì ¸ë‹¤ ì‚¬ìš©í•˜ê¸° ì‰½ê²Œ í•´ì£¼ëŠ” ê¸°ëŠ¥

### ReduceLROnPlateau - Reduce learning rate when a metric has stopped improving.

* scalar - a single number or value
* vector - an array of numbers, either in a row or in a column, identified by an index
* matrix - a 2-D array of numbers, where each element is identified by two indeces 

### EarlyStopping - 
ë„ˆë¬´ ë§ì€ Epoch ì€ overfitting ì„ ì¼ìœ¼í‚¨ë‹¤. í•˜ì§€ë§Œ ë„ˆë¬´ ì ì€ Epoch ì€ underfitting ì„ ì¼ìœ¼í‚¨ë‹¤. 
Epoch ì„ ì •í•˜ëŠ”ë° ë§ì´ ì‚¬ìš©ë˜ëŠ” Early stopping ì€ ë¬´ì¡°ê±´ Epoch ì„ ë§ì´ ëŒë¦° í›„, íŠ¹ì • ì‹œì ì—ì„œ ë©ˆì¶”ëŠ” ê²ƒì´ë‹¤. 
ê·¸ íŠ¹ì •ì‹œì ì„ ì–´ë–»ê²Œ ì •í•˜ëŠëƒê°€ Early stopping ì˜ í•µì‹¬ì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ hold-out validation set ì—ì„œì˜ ì„±ëŠ¥ì´ ë”ì´ìƒ ì¦ê°€í•˜ì§€ ì•Šì„ ë•Œ í•™ìŠµì„ ì¤‘ì§€ì‹œí‚¤ê²Œ ëœë‹¤. 


Earlystopping í´ë˜ìŠ¤ì˜ êµ¬ì„± ìš”ì†Œ

Performance measure: ì–´ë–¤ ì„±ëŠ¥ì„ monitoring í•  ê²ƒì¸ê°€?
Trigger: ì–¸ì œ training ì„ ë©ˆì¶œ ê²ƒì¸ê°€?


hist = model.fit(train_x, train_y, nb_epoch=10,  

                 batch_size=10, verbose=2, validation_split=0.2,   

                 callbacks=[early_stopping])  

* verbose logging - In software, verbose logging is the practice of recording to a persistent medium as much information as you possibly can about events that occur while the software runs
if you enable verbose > 0, that printing to the screen is generally a very slow process. 
 The algorithm may run an order of magnitude slower, or more, with verbose enabled. 


### ModelCheckpoint- 

Early stopping ê°ì²´ì— ì˜í•´ íŠ¸ë ˆì´ë‹ì´ ì¤‘ì§€ë˜ì—ˆì„ ë•Œ, 
ê·¸ ìƒíƒœëŠ” ì´ì „ ëª¨ë¸ì— ë¹„í•´ ì¼ë°˜ì ìœ¼ë¡œ validation error ê°€ ë†’ì€ ìƒíƒœì¼ ê²ƒì´ë‹¤. 
ë”°ë¼ì„œ, Earlystopping ì„ í•˜ëŠ” ê²ƒì€ íŠ¹ì • ì‹œì ì— ëª¨ë¸ì˜ íŠ¸ë ˆì´ë‹ì„ ë©ˆì¶¤ìœ¼ë¡œì¨, 
ëª¨ë¸ì˜ validation error ê°€ ë” ì´ìƒ ë‚®ì•„ì§€ì§€ ì•Šë„ë¡ ì¡°ì ˆí•  ìˆ˜ëŠ” ìˆê² ì§€ë§Œ, ì¤‘ì§€ëœ ìƒíƒœê°€ ìµœê³ ì˜ ëª¨ë¸ì€ ì•„ë‹ ê²ƒì´ë‹¤. 
ë”°ë¼ì„œ ê°€ì¥ validation performance ê°€ ì¢‹ì€ ëª¨ë¸ì„ ì €ì¥í•˜ëŠ” ê²ƒì´ í•„ìš”í•œë°, 
keras ì—ì„œëŠ” ì´ë¥¼ ìœ„í•´ ModelCheckpoint ë¼ê³  í•˜ëŠ” ê°ì²´ë¥¼ ì¡´ì¬í•œë‹¤. ì´ ê°ì²´ëŠ” validation error ë¥¼ ëª¨ë‹ˆí„°ë§í•˜ë©´ì„œ, 
ì´ì „ epoch ì— ë¹„í•´ validation performance ê°€ ì¢‹ì€ ê²½ìš°, 
ë¬´ì¡°ê±´ ì´ ë•Œì˜ parameter ë“¤ì„ ì €ì¥í•œë‹¤. ì´ë¥¼ í†µí•´ íŠ¸ë ˆì´ë‹ì´ ì¤‘ì§€ë˜ì—ˆì„ ë•Œ,
ê°€ì¥ validation performance ê°€ ë†’ì•˜ë˜ ëª¨ë¸ì„ ë°˜í™˜í•  ìˆ˜ ìˆë‹¤. 


### TensorBoard- í…ì„œë³´ë“œëŠ” ë¨¸ì‹ ëŸ¬ë‹ ì‹¤í—˜ì— í•„ìš”í•œ ì‹œê°í™” ë° ë„êµ¬ë¥¼ ì œê³µ



### yolov3Tiny 

Tiny-yolov3 is a simplified version of YOLOv3, 
which has a much smaller number of convolution layers than YOLOv3, 
which means that tiny-yolov3 does not need to occupy a large amount of memory,
reducing the need for hardware. And it also greatly speeds up detection, 
but lost some of the detection accuracy.



### YoloLoss
YOLOëŠ” ê° grid cellë§ˆë‹¤ ë‹¤ìˆ˜ì˜ bounding boxesë¥¼ ì˜ˆì¸¡í•˜ì§€ë§Œ
true positiveì— ëŒ€í•œ lossë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•´ íƒì§€ëœ ê°ì²´ë¥¼ ê°€ì¥ ì˜ í¬í•¨í•˜ëŠ” box í•˜ë‚˜ë¥¼ ì„ íƒí•´ì•¼ í•œë‹¤. 
ì´ë¥¼ ìœ„í•´ ground truthì™€ IOUë¥¼ ê³„ì‚°í•˜ì—¬ ê°€ì¥ ë†’ì€ IOUë¥¼ ê°€ì§„ í•˜ë‚˜ë¥¼ ì„ íƒí•œë‹¤. 
ì´ë¡œì¨ í¬ê¸°ë‚˜ ê°€ë¡œ, ì„¸ë¡œ ë¹„ìœ¨ì— ëŒ€í•´ ë” ì¢‹ì€ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤.

### train ì—ì„œ fake_dataë¥¼ í•™ìŠµì‹œí‚¤ëŠ” ì´ìœ  

Even worse it may (and likely is) trying to generalize off of your incorrect examples,
which will lessen the effect your real examples have. 
Essentially, you are just dampening your training set with noise.
--> fake_dataë„ correct í•™ìŠµì˜ ì¼ë¶€ì´ë‹¤. 


### trainì—ì„œ suffleë¥¼ ì‚¬ìš©í•˜ëŠ” ì´ìœ  

ìˆœì°¨ì ìœ¼ë¡œ í•™ìŠµì„ ì‹œí‚¤ì§€ë§Œ ìˆœì„œë¥¼ ì„ì˜ë¡œ ë°”ê¾¸ì–´ì„œ ë¬´ì‘ìœ„ë¡œ ë‹¤ì‹œ í•™ìŠµì„ ì‹œí‚¤ëŠ” ê³¼ì •ì´ í•„ìš”í•˜ë‹¤ .


The process of training a neural network is to find the minimum value of a loss function â„’ğ‘‹(ğ‘Š),
where ğ‘Š represents a matrix (or several matrices) of weights between neurons 
and ğ‘‹ represents the training dataset. 
I use a subscript for ğ‘‹ to indicate that our minimization of â„’ occurs only over the weights ğ‘Š 
(that is, we are looking for ğ‘Š such that â„’ is minimized) while ğ‘‹ is fixed.


### trainì—ì„œ prefetch() ë¥¼ ì‚¬ìš©í•˜ëŠ” ì´ìœ  
tf.data íŒŒì´í”„ë¼ì¸ì„ ì‚¬ìš©í•´ HDFSì— ìˆëŠ” í•™ìŠµ ë°ì´í„°ë¥¼ ì§ì ‘ ì½ì–´ì˜¤ëŠ” ë°©ë²•ì„ ì œê³µí•œë‹¤. 
Dataset.prefetch() ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ë©´ í•™ìŠµ ë°ì´í„°ë¥¼ ë‚˜ëˆ ì„œ ì½ì–´ì˜¤ê¸° ë•Œë¬¸ì— 
ì²« ë²ˆì§¸ ë°ì´í„°ë¥¼ GPUì—ì„œ í•™ìŠµí•˜ëŠ” ë™ì•ˆ ë‘ ë²ˆì§¸ ë°ì´í„°ë¥¼ CPUì—ì„œ ì¤€ë¹„í•  ìˆ˜ ìˆì–´ ë¦¬ì†ŒìŠ¤ì˜ ìœ íœ´ ìƒíƒœë¥¼ ì¤„ì¼ ìˆ˜ ìˆë‹¤.


### machine learning ì—ì„œ validation setë¥¼ ì‚¬ìš©í•˜ëŠ” ì´ìœ  
ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ê¸° ìœ„í•´ì„œ
trainingì„ í•œ í›„ì— ë§Œë“¤ì–´ì§„ ëª¨í˜•ì´ ì˜ ì˜ˆì¸¡ì„ í•˜ëŠ”ì§€ ê·¸ ì„±ëŠ¥ì„ í‰ê°€í•˜ê¸° ìœ„í•´ì„œ ì‚¬ìš©í•©ë‹ˆë‹¤. 

ë°ì´í„°ë¥¼ êµ¬í•˜ê³  ë‚˜ì„œ ë¶„ì„ì„ ì‹œì‘í•  ë•Œ ëŒ€ë¶€ë¶„ ì²˜ìŒ í•˜ëŠ” ì‘ì—…ì€ ë°ì´í„°ë¥¼ 3ë“±ë¶„ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ì‘ì—…ì´ë‹¤.
Train Data
ë¶„ì„ ëª¨ë¸ì„ ë§Œë“¤ê¸° ìœ„í•œ í•™ìŠµìš© ë°ì´í„°ì´ë‹¤.
Validation Data
ì—¬ëŸ¬ ë¶„ì„ ëª¨ë¸ ì¤‘ ì–´ë–¤ ëª¨ë¸ì´ ì í•©í•œì§€ ì„ íƒí•˜ê¸° ìœ„í•œ ê²€ì¦ìš© ë°ì´í„°ì´ë‹¤.
Test Data
ìµœì¢…ì ìœ¼ë¡œ ì„ íƒëœ ë¶„ì„ ëª¨ë¸ì´ ì–¼ë§ˆë‚˜ ì˜ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸í•˜ê¸° ìœ„í•œ ê²°ê³¼ìš© ë°ì´í„°ì´ë‹¤.

validation dataset = training datasetì—ì„œ ì¶”ì¶œëœ ê°€ìƒì˜ datasetì´ë‹¤.



### Transfer Learning? 

- transfer learning ì´ë€ ë¨¸ì‹  ëŸ¬ë‹ ê¸°ìˆ ë¡œì„œ í•œ taskì—ì„œ model ì´ train ê²ƒì´ 
ë‹¤ì‹œ ë‘ë²ˆì§¸ë¡œ ê´€ë ¨ëœ taskì—ì„œ ì¬ê³  ë˜ëŠ”ê²ƒì„ ë§í•œë‹¤. ì‰½ê²Œ ë§í•´, ê¸°ì¡´ì˜ ë§Œë“¤ì–´ì§„ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬
ìƒˆë¡œìš´ ëª¨ë¸ì„ ë§Œë“¤ì‹œ í•™ìŠµì„ ë¹ ë¥´ê²Œ í•˜ì—¬, ì˜ˆì¸¡ì„ ë” ë†’ì´ëŠ” ë°©ë²•ì„ ë§í•œë‹¤. 

- ì™œ ì‚¬ìš©í•˜ëŠ”ì§€ ? 
1. ì‹¤ì§ˆì ìœ¼ë¡œ Convolution networkì„ ì²˜ìŒë¶€í„° í•™ìŠµì‹œí‚¤ëŠ” ì¼ì€ ë§ì§€ ì•Šë‹¤.
ëŒ€ë¶€ë¶„ì˜ ë¬¸ì œëŠ” ì´ë¯¸ í•™ìŠµëœ ëª¨ë¸ì„ ì‚¬ìš©í•´ì„œ ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
2. ë³µì¡í•œ ëª¨ë¸ì¼ìˆ˜ë¡ í•™ìŠµì‹œí‚¤ê¸° ì–´ë µë‹¤.
ì–´ë–¤ ëª¨ë¸ì€ 2ì£¼ì •ë„ ê±¸ë¦´ìˆ˜ ìˆìœ¼ë©°, ë¹„ì‹¼ GPU ì—¬ëŸ¬ëŒ€ë¥¼ ì‚¬ìš©í•˜ê¸°ë„ í•©ë‹ˆë‹¤.
3. layersì˜ ê°¯ìˆ˜, activation, hyper parametersë“±ë“± ê³ ë ¤í•´ì•¼ í•  ì‚¬í•­ë“¤ì´ ë§ìœ¼ë©°, 
ì‹¤ì§ˆì ìœ¼ë¡œ ì²˜ìŒë¶€í„° í•™ìŠµì‹œí‚¤ë ¤ë©´ ë§ì€ ì‹œë„ê°€ í•„ìš”í•˜ë‹¤. 

ê²°ë¡ ì ìœ¼ë¡œ ì´ë¯¸ ì˜ í›ˆë ¨ëœ ëª¨ë¸ì´ ìˆê³ , íŠ¹íˆ í•´ë‹¹ ëª¨ë¸ê³¼ ìœ ì‚¬í•œ ë¬¸ì œë¥¼ í•´ê²°ì‹œ transfer leariningì„ ì‚¬ìš©í•œë‹¤. 

### Overfitting ? (ê³¼ì í•© )

í•™ìŠµ ë°ì´í„°ë¥¼ ê³¼í•˜ê²Œ ì˜ í•™ìŠµí•˜ëŠ” ê²ƒì„ ë§í•œë‹¤. 
ì¼ë°˜ì ìœ¼ë¡œ í•™ìŠµë°ì´í„°ëŠ” ì‹¤ì œ ë°ì´í„°ì˜ ë¶€ë¶„ì§‘í•©ì¸ ê²½ìš°ê°€ ëŒ€ë¶€ë¶„ì´ë‹¤. 
í•™ìŠµ ëª¨ë¸ì´ training setì— ì¡´ì¬í•˜ëŠ” noiseê¹Œì§€ í•™ìŠµí•˜ì—¬ test setì—ì„œëŠ” ì •í™•ë„ê°€ ë‚®ì€ ìƒí™© 

### Regularizaion  ? (ì¼ë°˜í™” )

overfitting ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ ëŒ€ì±… 

cost function í˜¹ì€ error function ì´ ì‘ì•„ì§€ëŠ” ìª½ìœ¼ë¡œ ì¶”ë¡ ì„ í•  ê²½ìš°ì—,
ë‹¨ìˆœí•˜ê²Œ ì˜¤ì°¨ê°€ ì‘ì•„ì§€ëŠ” ìª½ìœ¼ë¡œë§Œ ì§„í–‰ì„ í•˜ë‹¤ ë³´ë©´ íŠ¹ì • ê°€ì¤‘ì¹˜ ê°’ë“¤ì´ ì»¤ì§€ë©´ì„œ ì˜¤íˆë ¤ ê²°ê³¼ë¥¼ ë‚˜ì˜ê²Œ í•˜ëŠ” ê²½ìš°ë„ ìˆë‹¤. 
ë”°ë¼ì„œ regularizationì„ ì‚¬ìš©í•˜ì—¬ ë” ì¢‹ì€ í•™ìŠµ ê²°ê³¼ë¥¼ ê°€ì ¸ ì˜¤ë„ë¡ í•œë‹¤. 

refularizationì—ëŠ” l1, l2 ê°€ ìˆë‹¤. 


### ì†ì‹¤í•¨ìˆ˜ ? 

ì†ì‹¤í•¨ìˆ˜ (Loss function) ,ë¹„ìš©í•¨ìˆ˜ (costfunction)ëŠ” ì‹¤ì œê°’ê³¼ ì˜ˆì¸¡ê°’ì´ ì°¨ì´ê°€ ë‚¬ì„ ë•Œ, ê·¸ ì˜¤ì°¨ê°€ ì–¼ë§ˆì¸ì§€ ê³„ì‚°í•´ì£¼ëŠ” í•¨ìˆ˜ì´ë‹¤. 
ìš°ë¦¬ëŠ” ì—¬ê¸°ì„œ ì˜¤ì°¨ë¥¼ ì¤„ì´ë„ë¡ ì‹ ê²½ë§ì„ ê³„ì†í•´ì„œ í•™ìŠµì‹œì¼œë‚˜ê°€ëŠ” ê²ƒì´ ëª©ì ì´ë‹¤. ì´ ì˜¤ì°¨ë¥¼ ê³„ì‚°í•˜ë„ë¡ ë„ì™€ì£¼ëŠ” í•¨ìˆ˜ê°€ ì†ì‹¤í•¨ìˆ˜ì´ë‹¤. 
ì†ì‹¤í•¨ìˆ˜ë¥¼ ê³„ì‚°í•˜ëŠ” ë°©ë²•ì—ëŠ” í‰ê· ì œê³±ì˜¤ì°¨, êµì°¨ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨ ë°©ë²• ë“±ì´ ìˆë‹¤. 

"""


from absl import app, flags, logging
from absl.flags import FLAGS

import tensorflow as tf
import numpy as np
import cv2
from tensorflow.keras.callbacks import (
    ReduceLROnPlateau,
    EarlyStopping,
    ModelCheckpoint,
    TensorBoard
)
from yolov3_tf2.models import (
    YoloV3, YoloV3Tiny, YoloLoss,
    yolo_anchors, yolo_anchor_masks,
    yolo_tiny_anchors, yolo_tiny_anchor_masks
)
from yolov3_tf2.utils import freeze_all
import yolov3_tf2.dataset as dataset



flags.DEFINE_string('dataset', '', 'path to dataset')
flags.DEFINE_string('val_dataset', '', 'path to validation dataset')
flags.DEFINE_boolean('tiny', False, 'yolov3 or yolov3-tiny')
flags.DEFINE_string('weights', './checkpoints/yolov3.tf',
                    'path to weights file')
flags.DEFINE_string('classes', './data/coco.names', 'path to classes file')
flags.DEFINE_enum('mode', 'fit', ['fit', 'eager_fit', 'eager_tf'],
                  'fit: model.fit, '
                  'eager_fit: model.fit(run_eagerly=True), '
                  'eager_tf: custom GradientTape')
flags.DEFINE_enum('transfer', 'none',
                  ['none', 'darknet', 'no_output', 'frozen', 'fine_tune'],
                  'none: Training from scratch, '
                  'darknet: Transfer darknet, '
                  'no_output: Transfer all but output, '
                  'frozen: Transfer and freeze all, '
                  'fine_tune: Transfer all and freeze darknet only')
flags.DEFINE_integer('size', 416, 'image size')
flags.DEFINE_integer('epochs', 2, 'number of epochs')
flags.DEFINE_integer('batch_size', 8, 'batch size')
flags.DEFINE_float('learning_rate', 1e-3, 'learning rate')
flags.DEFINE_integer('num_classes', 80, 'number of classes in the model')
flags.DEFINE_integer('weights_num_classes', None, 'specify num class for `weights` file if different, '
                     'useful in transfer learning with different number of classes')


def main(_argv):
    physical_devices = tf.config.experimental.list_physical_devices('GPU')
    for physical_device in physical_devices:
        tf.config.experimental.set_memory_growth(physical_device, True)

    if FLAGS.tiny: # yolov3 tiny ì¼ ê²½ìš° 
        model = YoloV3Tiny(FLAGS.size, training=True,
                           classes=FLAGS.num_classes)
        anchors = yolo_tiny_anchors
        anchor_masks = yolo_tiny_anchor_masks
    else: # yolov3ì¼ ê²½ìš° model, anchor, anchor_mask ì •í•˜ê¸° 
        model = YoloV3(FLAGS.size, training=True, classes=FLAGS.num_classes)
        anchors = yolo_anchors
        anchor_masks = yolo_anchor_masks

    if FLAGS.dataset: # dataset ì¼ ê²½ìš° 
        train_dataset = dataset.load_tfrecord_dataset(
            FLAGS.dataset, FLAGS.classes, FLAGS.size) # ì´ë¯¸ì§€, ë¼ë²¨ë§, ì´ë¯¸ì§€ ì‚¬ì´ì¦ˆ ë¡œë“œ 
    else:
        train_dataset = dataset.load_fake_dataset() # fake_dataset ë¡œë“œí•˜ì—¬ í•™ìŠµ
    train_dataset = train_dataset.shuffle(buffer_size=512)  # ë°ì´í„°ì…‹ì„ ë¬´ì‘ìœ„ë¡œ shuffle ì‹œí‚¤ê¸° 
    train_dataset = train_dataset.batch(FLAGS.batch_size) # ì¼ì • batch í¬ê¸° ë§Œí¼ í•™ìŠµ ì‹œí‚¤ê¸° 
    train_dataset = train_dataset.map(lambda x, y: (
        dataset.transform_images(x, FLAGS.size),
        dataset.transform_targets(y, anchors, anchor_masks, FLAGS.size))) # ë°ì´í„°ì…‹ì˜ ì´ë¯¸ì§€ì™€ targetë¥¼ ë³€í˜•ì‹œí‚¤ê¸° 
    train_dataset = train_dataset.prefetch(
        buffer_size=tf.data.experimental.AUTOTUNE) # prefetchë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµ ë°ì´í„°ë¥¼ ë‚˜ëˆ„ì–´ì„œ ì „ì†¡ / ë¦¬ì†ŒìŠ¤ì˜ ìœ íœ´ìƒíƒœ ì¤„ì´ê¸° 

    if FLAGS.val_dataset: # val_datasetì¼ ê²½ìš°(training datasetì—ì„œ ì¶”ì¶œëœ ê°€ìƒì˜ dataset) 
        val_dataset = dataset.load_tfrecord_dataset(
            FLAGS.val_dataset, FLAGS.classes, FLAGS.size)
    else:
        val_dataset = dataset.load_fake_dataset() # datasetì—ì„œ fake data ê°€ì ¸ì˜¨ë‹¤. 
    val_dataset = val_dataset.batch(FLAGS.batch_size) # datasetì—ì„œ batch size ê°€ì ¸ì˜¨ë‹¤. 
    val_dataset = val_dataset.map(lambda x, y: ( # ì´ë¯¸ì§€ë¥¼ ë³€í˜•ì‹œí‚¨ë‹¤. 
        dataset.transform_images(x, FLAGS.size),
        dataset.transform_targets(y, anchors, anchor_masks, FLAGS.size)))

    # Configure the model for transfer learning
    # transfer í•™ìŠ´ì„ ì§„í–‰í•˜ê¸° ìœ„í•´ ëª¨ë¸ ì„¤ì •í•˜ê¸° 
    if FLAGS.transfer == 'none':
        pass  # Nothing to do
    elif FLAGS.transfer in ['darknet', 'no_output']:
        # Darknet transfer is a special case that works
        # with incompatible number of classes

        # top layers ë¥¼ ë‹¤ì‹œ ì„¤ì •í•˜ê¸° 
        if FLAGS.tiny: # tiny ë°ì´í„°ë¥¼ í•™ìŠµ ì‹œí‚¬ ê²½ìš° 
            model_pretrained = YoloV3Tiny(
                FLAGS.size, training=True, classes=FLAGS.weights_num_classes or FLAGS.num_classes)
        else: # yolov3 ë°ì´í„°ë¥¼ í•™ìŠµ ì‹œí‚¬ ê²½ìš° 
            model_pretrained = YoloV3(
                FLAGS.size, training=True, classes=FLAGS.weights_num_classes or FLAGS.num_classes)
        model_pretrained.load_weights(FLAGS.weights)

        if FLAGS.transfer == 'darknet': # transfer ì´ darknet ì¼ ê²½ìš° 
            model.get_layer('yolo_darknet').set_weights( # yolo_darknet layerì— ê°€ì¤‘ì¹˜ ë¶€ì—¬ 
                model_pretrained.get_layer('yolo_darknet').get_weights())
            freeze_all(model.get_layer('yolo_darknet')) # freeze ì‹œì¼œì„œ í•™ìŠµ íš¨ìœ¨ ì¦ëŒ€ 

        elif FLAGS.transfer == 'no_output': # transfer ì´ no_output ì¼ ê²½ìš° 
            for l in model.layers:
                if not l.name.startswith('yolo_output'):
                    l.set_weights(model_pretrained.get_layer(
                        l.name).get_weights())
                    freeze_all(l)

    else:
        # ëª¨ë“  ë‹¤ë¥¸ transfer ì˜ ê²½ìš° matching classes ê°€ í•„ìš”í•˜ë‹¤ 
        model.load_weights(FLAGS.weights)
        if FLAGS.transfer == 'fine_tune': # transfer ì´ fine_tuneì¼ ê²½ìš° 
            # freeze darknet and fine tune other layers
            darknet = model.get_layer('yolo_darknet')
            freeze_all(darknet)
        elif FLAGS.transfer == 'frozen': # transfer ì´ frozen ì¼ ê²½ìš° 
            # freeze everything
            freeze_all(model) 

    optimizer = tf.keras.optimizers.Adam(lr=FLAGS.learning_rate) # optimizer ì„¤ì • 
    loss = [YoloLoss(anchors[mask], classes=FLAGS.num_classes) # ì†ì‹¤ í•¨ìˆ˜ ì„¤ì • 
            for mask in anchor_masks]

    if FLAGS.mode == 'eager_tf':
        # Eager mode ë””ë²„ê¹…ì— ì í•©í•˜ë‹¤. 
        # Non eager graph mode ì‹¤ì œ trainê³¼ì •ì—ì„œ í•„ìš”ë¡œí•œë‹¤. 
        avg_loss = tf.keras.metrics.Mean('loss', dtype=tf.float32) # loss ì˜ í‰ê· ê°’ ì„ rankê°€ 3ì¸ metrix, tf.float32 í˜•ìœ¼ë¡œ  ì„ ì–¸ 
        avg_val_loss = tf.keras.metrics.Mean('val_loss', dtype=tf.float32) # val_loss ì„ rankê°€ 3ì¸ metrix, tf.float32 í˜•ìœ¼ë¡œ ì„ ì–¸ 

        for epoch in range(1, FLAGS.epochs + 1): # epochë¥¼ ì„¤ì •í•˜ì—¬ í•™ìŠµì„ ë°˜ë³µì‹œí‚¨ë‹¤. 
            for batch, (images, labels) in enumerate(train_dataset): # batch ì‚¬ì´ì¦ˆ, image,labelë¥¼ train_dataset ì˜ í¬ê¸°ë§Œí¼ ëŒë¦°ë‹¤. 
                with tf.GradientTape() as tape: # GradientTapeì¸ tape ë¥¼ ì‚¬ìš©í•˜ì—¬ ê²½ì‚¬í•˜ê°•ë²• ì‚¬ìš©í•˜ì—¬ ì†ì‹¤í•¨ìˆ˜ì˜ ìµœì†Œê°’ì„ ì°¾ëŠ”ë‹¤. 
                    outputs = model(images, training=True)
                    regularization_loss = tf.reduce_sum(model.losses) # reduce_sum -> í•©ì„ êµ¬í•œë‹¤. cf) reduce_mean = í‰ê· ì„ êµ¬í•œë‹¤. 
                    pred_loss = []  # predict boxì˜ ì†ì‹¤ê°’ì„ ë„£ê³ ì pred_loss ì´ˆê¸°í™” 
                    for output, label, loss_fn in zip(outputs, labels, loss):
                        pred_loss.append(loss_fn(label, output)) # pred_loss ì— 
                    total_loss = tf.reduce_sum(pred_loss) + regularization_loss 
                # gradient - ê²½ì‚¬ í•˜ê°• ì•Œê³ ë¦¬ì¦˜ -> Convex ì†ì‹¤ í•¨ìˆ˜ì—ì„œ ìµœì†Œí™” (W,b)ê°’ì„ êµ¬í•œë‹¤. ë¯¸ë¶„ì„ ì‚¬ìš©í•˜ì—¬ ì ‘ì„ ì˜ ê¸°ìš¸ê¸°ë¥¼ êµ¬í•´ ê°’ì„ ì¡°ì •í•˜ì—¬ minimizeê°’ì„ ì°¾ëŠ”ë‹¤. 
                grads = tape.gradient(total_loss, model.trainable_variables)
                optimizer.apply_gradients(
                    zip(grads, model.trainable_variables))

                logging.info("{}_train_{}, {}, {}".format(
                    epoch, batch, total_loss.numpy(),
                    list(map(lambda x: np.sum(x.numpy()), pred_loss))))
                avg_loss.update_state(total_loss)

            for batch, (images, labels) in enumerate(val_dataset):
                outputs = model(images)
                regularization_loss = tf.reduce_sum(model.losses)
                pred_loss = []
                for output, label, loss_fn in zip(outputs, labels, loss):
                    pred_loss.append(loss_fn(label, output))
                total_loss = tf.reduce_sum(pred_loss) + regularization_loss

                logging.info("{}_val_{}, {}, {}".format(
                    epoch, batch, total_loss.numpy(),
                    list(map(lambda x: np.sum(x.numpy()), pred_loss))))
                avg_val_loss.update_state(total_loss)

            logging.info("{}, train: {}, val: {}".format(
                epoch,
                avg_loss.result().numpy(),
                avg_val_loss.result().numpy()))

            avg_loss.reset_states()
            avg_val_loss.reset_states()
            model.save_weights(
                'checkpoints/yolov3_train_{}.tf'.format(epoch)) # modelì— ê°€ì¤‘ì¹˜ë¥¼ ì €ì¥í•œë‹¤ .
    else:
        model.compile(optimizer=optimizer, loss=loss,
                      run_eagerly=(FLAGS.mode == 'eager_fit'))

        callbacks = [
            ReduceLROnPlateau(verbose=1),
            EarlyStopping(patience=3, verbose=1),
            ModelCheckpoint('checkpoints/yolov3_train_{epoch}.tf',
                            verbose=1, save_weights_only=True),
            TensorBoard(log_dir='logs')
        ]

        history = model.fit(train_dataset,
                            epochs=FLAGS.epochs,
                            callbacks=callbacks,
                            validation_data=val_dataset)


if __name__ == '__main__':
    try:
        app.run(main)
    except SystemExit:
        pass
